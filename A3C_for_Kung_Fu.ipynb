{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# A3C for Kung Fu"
   ],
   "metadata": {
    "id": "dIo6Zkp7U1Hq"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 0 - Installing the required packages and importing the libraries"
   ],
   "metadata": {
    "id": "pz8ogVxGVB6b"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Installing Gymnasium"
   ],
   "metadata": {
    "id": "CqN2IEX1VKzi"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dbnq3XpoKa_7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "773f7e28-3fba-47f9-919a-a70b53ca2dec",
    "ExecuteTime": {
     "end_time": "2024-11-19T17:50:28.657874Z",
     "start_time": "2024-11-19T17:50:26.926308Z"
    }
   },
   "source": [
    "!pip install gymnasium\n",
    "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
    "!pip install ale-py\n",
    "!apt-get install -y swig\n",
    "!pip install gymnasium[box2d]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /Users/tan/PycharmProjects/.venv/lib/python3.12/site-packages (1.0.0)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/tan/PycharmProjects/.venv/lib/python3.12/site-packages (from gymnasium) (2.1.3)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/tan/PycharmProjects/.venv/lib/python3.12/site-packages (from gymnasium) (3.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/tan/PycharmProjects/.venv/lib/python3.12/site-packages (from gymnasium) (4.12.2)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/tan/PycharmProjects/.venv/lib/python3.12/site-packages (from gymnasium) (0.0.4)\r\n",
      "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /Users/tan/PycharmProjects/.venv/lib/python3.12/site-packages (1.0.0)\r\n",
      "\u001B[33mWARNING: gymnasium 1.0.0 does not provide the extra 'accept-rom-license'\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: numpy>=1.21.0 in /Users/tan/PycharmProjects/.venv/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (2.1.3)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/tan/PycharmProjects/.venv/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (3.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/tan/PycharmProjects/.venv/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/tan/PycharmProjects/.venv/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\r\n",
      "Requirement already satisfied: ale-py>=0.9 in /Users/tan/PycharmProjects/.venv/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (0.10.1)\r\n",
      "Requirement already satisfied: ale-py in /Users/tan/PycharmProjects/.venv/lib/python3.12/site-packages (0.10.1)\r\n",
      "Requirement already satisfied: numpy>1.20 in /Users/tan/PycharmProjects/.venv/lib/python3.12/site-packages (from ale-py) (2.1.3)\r\n",
      "zsh:1: command not found: apt-get\r\n",
      "zsh:1: no matches found: gymnasium[box2d]\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importing the libraries"
   ],
   "metadata": {
    "id": "BrsNHNQqVZLK"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ho_25-9_9qnu",
    "ExecuteTime": {
     "end_time": "2024-11-19T17:50:29.667071Z",
     "start_time": "2024-11-19T17:50:28.659665Z"
    }
   },
   "source": [
    "import cv2\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributions as distributions\n",
    "from torch.distributions import Categorical\n",
    "import ale_py\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium import ObservationWrapper"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 1 - Building the AI"
   ],
   "metadata": {
    "id": "VF6EFSGUVlk2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating the architecture of the Neural Network"
   ],
   "metadata": {
    "id": "qyNc8cxbZCYP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Network(nn.Module):\n",
    "  def __init__(self, action_size):\n",
    "    super(Network, self).__init__()\n",
    "    self.conv1 = torch.nn.Conv2d(in_channels = 4, out_channels = 32, kernel_size = (3,3), stride = 2)\n",
    "    self.conv2 = torch.nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = (3,3), stride = 2)\n",
    "    self.conv3 = torch.nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = (3,3), stride = 2)\n",
    "    self.flatten = torch.nn.Flatten()\n",
    "    self.fc1 = torch.nn.Linear(512, 128)\n",
    "    self.fc2a = torch.nn.Linear(128, action_size)\n",
    "    self.fc2s = torch.nn.Linear(128, 1)\n",
    "\n",
    "  def forward(self, state):\n",
    "    x = F.relu(self.conv1(state))\n",
    "    x = F.relu(self.conv2(x))\n",
    "    x = F.relu(self.conv3(x))\n",
    "    x = self.flatten(x)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    action_values = self.fc2a(x)\n",
    "    state_values = self.fc2s(x)[0]\n",
    "    return action_values, state_values"
   ],
   "metadata": {
    "id": "iX9HAKO2BKHA",
    "ExecuteTime": {
     "end_time": "2024-11-19T17:50:29.670592Z",
     "start_time": "2024-11-19T17:50:29.667822Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "af37jzv3BzZt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2 - Training the AI"
   ],
   "metadata": {
    "id": "eF5bETqbZbCG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setting up the environment"
   ],
   "metadata": {
    "id": "3C2ydyKLZgaK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class PreprocessAtari(ObservationWrapper):\n",
    "\n",
    "  def __init__(self, env, height = 42, width = 42, crop = lambda img: img, dim_order = 'pytorch', color = False, n_frames = 4):\n",
    "    super(PreprocessAtari, self).__init__(env)\n",
    "    self.img_size = (height, width)\n",
    "    self.crop = crop\n",
    "    self.dim_order = dim_order\n",
    "    self.color = color\n",
    "    self.frame_stack = n_frames\n",
    "    n_channels = 3 * n_frames if color else n_frames\n",
    "    obs_shape = {'tensorflow': (height, width, n_channels), 'pytorch': (n_channels, height, width)}[dim_order]\n",
    "    self.observation_space = Box(0.0, 1.0, obs_shape)\n",
    "    self.frames = np.zeros(obs_shape, dtype = np.float32)\n",
    "\n",
    "  def reset(self):\n",
    "    self.frames = np.zeros_like(self.frames)\n",
    "    obs, info = self.env.reset()\n",
    "    self.update_buffer(obs)\n",
    "    return self.frames, info\n",
    "\n",
    "  def observation(self, img):\n",
    "    img = self.crop(img)\n",
    "    img = cv2.resize(img, self.img_size)\n",
    "    if not self.color:\n",
    "      if len(img.shape) == 3 and img.shape[2] == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = img.astype('float32') / 255.\n",
    "    if self.color:\n",
    "      self.frames = np.roll(self.frames, shift = -3, axis = 0)\n",
    "    else:\n",
    "      self.frames = np.roll(self.frames, shift = -1, axis = 0)\n",
    "    if self.color:\n",
    "      self.frames[-3:] = img\n",
    "    else:\n",
    "      self.frames[-1] = img\n",
    "    return self.frames\n",
    "\n",
    "  def update_buffer(self, obs):\n",
    "    self.frames = self.observation(obs)\n",
    "\n",
    "def make_env():\n",
    "  env = gym.make(\"KungFuMasterDeterministic-v0\", render_mode = 'rgb_array')\n",
    "  env = PreprocessAtari(env, height = 42, width = 42, crop = lambda img: img, dim_order = 'pytorch', color = False, n_frames = 4)\n",
    "  return env\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "state_shape = env.observation_space.shape\n",
    "number_actions = env.action_space.n\n",
    "print(\"State shape:\", state_shape)\n",
    "print(\"Number actions:\", number_actions)\n",
    "print(\"Action names:\", env.env.env.env.get_action_meanings())\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gF756uIhRVcK",
    "outputId": "741d72d2-4594-4a0a-fa1a-bf69bc020e6c",
    "ExecuteTime": {
     "end_time": "2024-11-19T17:50:29.723308Z",
     "start_time": "2024-11-19T17:50:29.672022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape: (4, 42, 42)\n",
      "Number actions: 14\n",
      "Action names: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tan/PycharmProjects/.venv/lib/python3.12/site-packages/gymnasium/envs/registration.py:517: DeprecationWarning: \u001B[33mWARN: The environment KungFuMasterDeterministic-v0 is out of date. You should consider upgrading to version `v4`.\u001B[0m\n",
      "  logger.deprecation(\n",
      "A.L.E: Arcade Learning Environment (version 0.10.1+6a7e0ae)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initializing the hyperparameters"
   ],
   "metadata": {
    "id": "YgRlooBmC1hr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "learning_rate = 1e-4\n",
    "discount_factor = 0.99\n",
    "number_environments = 100"
   ],
   "metadata": {
    "id": "1BOsjciCU8kk",
    "ExecuteTime": {
     "end_time": "2024-11-19T17:50:29.725329Z",
     "start_time": "2024-11-19T17:50:29.723877Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementing the A3C class"
   ],
   "metadata": {
    "id": "Gg_LmSs9IoTX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Agent():\n",
    "\n",
    "  def __init__(self, action_size):\n",
    "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    self.action_size = action_size\n",
    "    self.network = Network(action_size).to(self.device)\n",
    "    self.optimizer = torch.optim.Adam(self.network.parameters(), lr = learning_rate)\n",
    "\n",
    "  def act(self, state):\n",
    "    if state.ndim == 3:\n",
    "      state = [state]\n",
    "    state = torch.tensor(state, dtype = torch.float32, device = self.device)\n",
    "    action_values, _ = self.network(state)\n",
    "    policy = F.softmax(action_values, dim = -1)\n",
    "    return np.array([np.random.choice(len(p), p = p) for p in policy.detach().cpu().numpy()])\n",
    "\n",
    "  def step(self, state, action, reward, next_state, done):\n",
    "    batch_size = state.shape[0]\n",
    "    state = torch.tensor(state, dtype = torch.float32, device = self.device)\n",
    "    next_state = torch.tensor(next_state, dtype = torch.float32, device = self.device)\n",
    "    reward = torch.tensor(reward, dtype = torch.float32, device = self.device)\n",
    "    done = torch.tensor(done, dtype = torch.bool, device = self.device).to(dtype = torch.float32)\n",
    "    action_values, state_value = self.network(state)\n",
    "    _, next_state_value = self.network(next_state)\n",
    "    target_state_value = reward + discount_factor * next_state_value * (1 - done)\n",
    "    advantage = target_state_value - state_value\n",
    "    probs = F.softmax(action_values, dim = -1)\n",
    "    logprobs = F.log_softmax(action_values, dim = -1)\n",
    "    entropy = -torch.sum(probs * logprobs, axis = -1)\n",
    "    batch_idx = np.arange(batch_size)\n",
    "    logp_actions = logprobs[batch_idx, action]\n",
    "    actor_loss = -(logp_actions * advantage.detach()).mean() - 0.001 * entropy.mean()\n",
    "    critic_loss = F.mse_loss(target_state_value.detach(), state_value)\n",
    "    total_loss = actor_loss + critic_loss\n",
    "    self.optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    self.optimizer.step()"
   ],
   "metadata": {
    "id": "DLEwI85n7buX",
    "ExecuteTime": {
     "end_time": "2024-11-19T17:50:29.729583Z",
     "start_time": "2024-11-19T17:50:29.725966Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initializing the A3C agent"
   ],
   "metadata": {
    "id": "7RnRukHDKFJ0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "agent = Agent(number_actions)"
   ],
   "metadata": {
    "id": "dEQpG80HnKbX",
    "ExecuteTime": {
     "end_time": "2024-11-19T17:50:30.203306Z",
     "start_time": "2024-11-19T17:50:29.730166Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluating our A3C agent on a single episode"
   ],
   "metadata": {
    "id": "oB5SpmoKP0aK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate(self, env, number_of_episodes=1 ):\n",
    "  episode_rewards = []\n",
    "  for _ in range (number_of_episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_rewards = 0\n",
    "    while True:\n",
    "      action = agent.act(state)\n",
    "      state, reward, done, info, _ = env.step(action[0])\n",
    "      total_rewards += reward\n",
    "      if done:\n",
    "        break\n",
    "    episode_rewards.append(total_rewards)\n",
    "  return episode_rewards"
   ],
   "metadata": {
    "id": "_ozqcnxwtQh8",
    "ExecuteTime": {
     "end_time": "2024-11-19T17:50:30.205775Z",
     "start_time": "2024-11-19T17:50:30.203865Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing multiple agents on multiple environments at the same time"
   ],
   "metadata": {
    "id": "jVSqiyjiQeMd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class EnvBatch:\n",
    "  def __init__(self, n_envs=10):\n",
    "    self.envs = [make_env() for _ in range(n_envs)]\n",
    "\n",
    "  def reset(self):\n",
    "    _states = []\n",
    "    for env in self.envs:\n",
    "     _states.append(env.reset()[0])\n",
    "    return np.array(_states)\n",
    "\n",
    "  def step(self, actions):\n",
    "    next_states, rewards, dones, infos, _ = map(np.array, zip(*[env.step(a) for env, a in zip(self.envs, actions)]))\n",
    "    for i in range (len(self.envs)):\n",
    "      if dones[i]:\n",
    "        next_states[i] = self.envs[i].reset()[0]\n",
    "    return next_states, rewards, dones, infos\n",
    "\n"
   ],
   "metadata": {
    "id": "O_vHQM9QwFME",
    "ExecuteTime": {
     "end_time": "2024-11-19T17:50:30.208673Z",
     "start_time": "2024-11-19T17:50:30.206435Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "PUfXNMKFz6_w"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the A3C agent"
   ],
   "metadata": {
    "id": "69WZWB4oRx1P"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import tqdm\n",
    "\n",
    "env_batch = EnvBatch(number_environments)\n",
    "batch_states = env_batch.reset()\n",
    "\n",
    "with tqdm.trange(0, 3001) as progress_bar:\n",
    "  for i in progress_bar:\n",
    "    batch_actions = agent.act(batch_states)\n",
    "    batch_next_states, batch_rewards, batch_dones, _ = env_batch.step(batch_actions)\n",
    "    batch_rewards *= 0.01\n",
    "    agent.step(batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones)\n",
    "    batch_states = batch_next_states\n",
    "    if i % 1000 == 0:\n",
    "      print(\"Average agent reward: \", np.mean(evaluate(agent, env,  number_of_episodes = 10)))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KVrH1IGU7nyQ",
    "outputId": "735e8898-e668-403a-a95d-5f30f3e9b788",
    "ExecuteTime": {
     "end_time": "2024-11-19T17:51:14.857007Z",
     "start_time": "2024-11-19T17:50:30.210147Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tan/PycharmProjects/.venv/lib/python3.12/site-packages/gymnasium/envs/registration.py:517: DeprecationWarning: \u001B[33mWARN: The environment KungFuMasterDeterministic-v0 is out of date. You should consider upgrading to version `v4`.\u001B[0m\n",
      "  logger.deprecation(\n",
      "  0%|          | 0/3001 [00:00<?, ?it/s]/var/folders/8r/ldg4kw7s72z8z3zfpvwvw__40000gn/T/ipykernel_91307/418256784.py:33: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([100])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  critic_loss = F.mse_loss(target_state_value.detach(), state_value)\n",
      "/var/folders/8r/ldg4kw7s72z8z3zfpvwvw__40000gn/T/ipykernel_91307/418256784.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  state = torch.tensor(state, dtype = torch.float32, device = self.device)\n",
      "  0%|          | 2/3001 [00:12<4:17:53,  5.16s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average agent reward:  570.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 251/3001 [00:39<07:16,  6.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m progress_bar:\n\u001B[1;32m      8\u001B[0m   batch_actions \u001B[38;5;241m=\u001B[39m agent\u001B[38;5;241m.\u001B[39mact(batch_states)\n\u001B[0;32m----> 9\u001B[0m   batch_next_states, batch_rewards, batch_dones, _ \u001B[38;5;241m=\u001B[39m \u001B[43menv_batch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_actions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m   batch_rewards \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.01\u001B[39m\n\u001B[1;32m     11\u001B[0m   agent\u001B[38;5;241m.\u001B[39mstep(batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones)\n",
      "Cell \u001B[0;32mIn[9], line 12\u001B[0m, in \u001B[0;36mEnvBatch.step\u001B[0;34m(self, actions)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, actions):\n\u001B[0;32m---> 12\u001B[0m   next_states, rewards, dones, infos, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(np\u001B[38;5;241m.\u001B[39marray, \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39m[env\u001B[38;5;241m.\u001B[39mstep(a) \u001B[38;5;28;01mfor\u001B[39;00m env, a \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menvs, actions)]))\n\u001B[1;32m     13\u001B[0m   \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m (\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menvs)):\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dones[i]:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 3 - Visualizing the results"
   ],
   "metadata": {
    "id": "7kG_YR9YdmUM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import imageio\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def show_video_of_model(agent, env):\n",
    "  state, _ = env.reset()\n",
    "  done = False\n",
    "  frames = []\n",
    "  while not done:\n",
    "    frame = env.render()\n",
    "    frames.append(frame)\n",
    "    action = agent.act(state)\n",
    "    state, reward, done, _, _ = env.step(action[0])\n",
    "  env.close()\n",
    "  imageio.mimsave('video.mp4', frames, fps=30)\n",
    "\n",
    "show_video_of_model(agent, env)\n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob('*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display(HTML(data='''<video alt=\"test\" autoplay\n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n",
    "\n",
    "show_video()"
   ],
   "metadata": {
    "id": "UGkTuO6DxZ6B",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "outputId": "a1d3afa6-a4f1-4c6c-a21f-5440fc9470f6",
    "ExecuteTime": {
     "end_time": "2024-11-19T17:51:14.858100Z",
     "start_time": "2024-11-19T17:51:14.858042Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
